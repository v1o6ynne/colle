{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class RAGSystem:\n",
    "    def __init__(self, metadata_dir):\n",
    "        self.metadata = []\n",
    "        self.main_page_spans = []\n",
    "        self.side_panel_spans = []\n",
    "        self.questions = []\n",
    "        self.questions_customized = []\n",
    "        self.answers = []\n",
    "        for filename in os.listdir(metadata_dir):\n",
    "            if filename.endswith('.json'):\n",
    "                file_path = os.path.join(metadata_dir, filename)\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    data = json.load(file)\n",
    "                    self.metadata.extend(data['dataset'])\n",
    "                    self.main_page_spans.extend(data['mainPageSpans'])\n",
    "                    self.side_panel_spans.extend(data['sidePanelSpans'])\n",
    "                    for item in data['dataset']:\n",
    "                        self.questions.extend(item.get('questions', []))\n",
    "                        self.questions_customized.extend(item.get('questions_customized', []))\n",
    "                        self.answers.extend(item.get('answers',[]))\n",
    "\n",
    "        self.tokenized_metadata = []\n",
    "        for item in self.metadata:\n",
    "            if 'text' not in item:\n",
    "                item['text'] = []\n",
    "            if isinstance(item['text'], list):\n",
    "                tokens = [nltk.word_tokenize(' '.join(text).lower()) for text in item['text']]\n",
    "                self.tokenized_metadata.extend(tokens)\n",
    "            else:\n",
    "                tokens = nltk.word_tokenize(item['text'].lower())\n",
    "                self.tokenized_metadata.append(tokens)\n",
    "                \n",
    "                \n",
    "        self.tokenized_questions_customized = [nltk.word_tokenize(question_customized.lower()) for question_customized in self.questions_customized]\n",
    "        self.tokenized_questions = [nltk.word_tokenize(question.lower()) for question in self.questions]\n",
    "        self.tokenized_answers = [nltk.word_tokenize(answer.lower()) for answer in self.answers]\n",
    "        self.model = Word2Vec(self.tokenized_metadata + self.tokenized_questions + self.tokenized_questions_customized + self.tokenized_answers, min_count=1)\n",
    "    \n",
    "    def retrieve(self, query, items, is_question=False, top_k=5):\n",
    "        query_tokens = nltk.word_tokenize(query.lower())\n",
    "        query_vector = self.get_vector(query_tokens)\n",
    "\n",
    "        similarity_scores = []\n",
    "        for tokens in items:\n",
    "            item_vector = self.get_vector(tokens)\n",
    "            similarity = cosine_similarity([query_vector], [item_vector])[0][0]\n",
    "            similarity_scores.append(similarity)\n",
    "\n",
    "        top_indices = sorted(range(len(similarity_scores)), key=lambda i: similarity_scores[i], reverse=True)[:top_k]\n",
    "        \n",
    "        if is_question:\n",
    "            retrieved_items = [self.questions[i] for i in top_indices]\n",
    "        else:\n",
    "            retrieved_items = [self.metadata[i] for i in top_indices]\n",
    "\n",
    "        return retrieved_items\n",
    "\n",
    "    def get_vector(self, tokens):\n",
    "        vectors = [self.model.wv[token] for token in tokens if token in self.model.wv]\n",
    "        if vectors:\n",
    "            return sum(vectors) / len(vectors)\n",
    "        else:\n",
    "            return self.model.wv['UNK']  # Return a default vector for unknown tokens\n",
    "    def remove_empty_objects(self, obj):\n",
    "        if isinstance(obj, dict):\n",
    "            cleaned_dict = {}\n",
    "            for k, v in obj.items():\n",
    "                cleaned_v = self.remove_empty_objects(v)\n",
    "                if cleaned_v is not None:\n",
    "                    cleaned_dict[k] = cleaned_v\n",
    "            return cleaned_dict if cleaned_dict else None\n",
    "        elif isinstance(obj, list):\n",
    "            cleaned_list = []\n",
    "            for elem in obj:\n",
    "                cleaned_elem = self.remove_empty_objects(elem)\n",
    "                if cleaned_elem is not None:\n",
    "                    cleaned_list.append(cleaned_elem)\n",
    "            return cleaned_list if cleaned_list else None\n",
    "        else:\n",
    "            return obj\n",
    "\n",
    "    def generate(self, query, query_questions, query_questions_customized, query_answers):\n",
    "        retrieved_items = self.retrieve(query, self.tokenized_metadata)\n",
    "        retrieved_items_questions = self.retrieve(query_questions, self.tokenized_questions, is_question=True)\n",
    "        retrieved_items_questions_customized = self.retrieve(query_questions_customized, self.tokenized_questions_customized, is_question=True)\n",
    "        retrieved_items_answers = self.retrieve(query_answers, self.tokenized_answers, is_question=True)\n",
    "\n",
    "        cleaned_retrieved_items = [self.remove_empty_objects(item) for item in retrieved_items]\n",
    "\n",
    "        new_retrieved_items = []\n",
    "        for item in cleaned_retrieved_items:\n",
    "            new_item = {\n",
    "                \"image\": item.get(\"image\", []),\n",
    "                \"caption\": item.get(\"caption\", []),\n",
    "                \"text\": [\" \".join(item.get(\"text\", []))],\n",
    "                \"questions\": retrieved_items_questions,\n",
    "                \"questions_customized\": retrieved_items_questions_customized,\n",
    "                \"answers\": retrieved_items_answers,\n",
    "                \"preference\": item.get(\"preference\", []),\n",
    "                \"note\": item.get(\"note\", []),\n",
    "                \"summary\": item.get(\"summary\", []),\n",
    "                \"subfigureJSON\": item.get(\"subfigureJSON\", [])\n",
    "            }\n",
    "            new_retrieved_items.append(new_item)\n",
    "\n",
    "        generated_metadata = {\n",
    "            \"dataset\": new_retrieved_items,\n",
    "            \"mainPageSpans\": self.main_page_spans,\n",
    "            \"sidePanelSpans\": self.side_panel_spans\n",
    "        }\n",
    "\n",
    "        return generated_metadata\n",
    "\n",
    "\n",
    "# Usage example\n",
    "metadata_dir = 'doc'\n",
    "rag_system = RAGSystem(metadata_dir)\n",
    "\n",
    "query = \"\"\"Analyze the given image and its associated metadata to answer the following questions:\n",
    "\n",
    "  1. What are the key visual elements in the image?\n",
    "  2. How does the caption describe the image?\n",
    "  4. What preferences or requests are mentioned in the metadata?\n",
    "  5. Are there any notable subfigures or annotations in the image?\n",
    "\n",
    "  Ignore the empty metadata and retrieve only the relevant information from the dataset to answer these questions. Provide a concise summary of the retrieved information.\n",
    "\"\"\"\n",
    "query_questions = \"\"\"\n",
    "  3. Are there any specific questions related to the image? If so, provide a summary of the questions.\n",
    "\"\"\"\n",
    "query_questions_customized = \"\"\"\n",
    "  Retrieve any custom questions related to the image.\n",
    "\"\"\"\n",
    "query_answers = \"\"\"\n",
    "  Retrieve the answers corresponding to the questions related to the image.\n",
    "\"\"\"\n",
    "generated_metadata = rag_system.generate(query, query_questions, query_questions_customized, query_answers)\n",
    "\n",
    "\n",
    "output_file = 'generated_metadata.json'\n",
    "with open(output_file, 'w') as file:\n",
    "    json.dump(generated_metadata, file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
