{
    "dataset": [
        {
            "image": [
                "https://dl.acm.org/cms/attachment/html/10.1145/3526113.3545703/assets/html/images/uist22-91-fig3.jpg"
            ],
            "caption": [
                "Authors can hover on segments of visual/audio timelines in CrossA11y to inspect the computed correspondence between the selected segment and segments in the other modality. Here, opaque segments in the audio track are the segments predicted by the system to match the segment in the video track."
            ],
            "text": [
                "By default, the video description pane displays visual segments with estimated accessibility scores lower than 0.35 (range 0-1). Authors can use the slider (Figure\u807d2 G) to surface more visual accessibility problems when making sure they covered everything, or fewer accessibility problems when prioritizing for a time constraint."
            ],
            "questions": [
                "[questions(descriptive):1]\nWhat color is used to represent the audio track in the figure?\n\n[questions(descriptive):2]\nWhat color is used to represent the visual track in the figure?\n\n[questions(descriptive):3]\nWhat action is being indicated by the cursor in the figure?\n\n[questions:4]\nIs there a distinct pattern or markers on the audio track that differs from the visual track?\n\n[questions:5]\nAre the intensities of the audio and visual tracks represented by variations in color saturation or brightness?",
                "[questions(descriptive):1]\nWhat color is used to represent the audio track in the figure?\n\n[questions(descriptive):2]\nWhat color is used to represent the visual track in the figure?\n\n[questions(descriptive):3]\nWhat action is being indicated by the cursor in the figure?\n\n[questions:4]\nIs there a distinct pattern or markers on the audio track that differs from the visual track?\n\n[questions:5]\nAre the intensities of the audio and visual tracks represented by variations in color saturation or brightness?",
                "1: [What color is used in the figure to indicate the predicted accessibility of each video description segment? (descriptive)]\n2: [How can authors preview the nearby narration in relation to the video description segments? (descriptive)]\n3: [What do the heights of the video description segments represent and how does that inform the authors? (descriptive)]\n4: [What options do authors have for each video description segment as shown in the figure?]\n5: [How are the video description segments and caption segments shown in relation to each other according to the figure?]\n6: [What does the alignment of video description segments with caption segments suggest about the design intent? (descriptive)]\n7: [In the context of the figure, what is the purpose of the \"save\"/\"edit\" and \"dismiss\" buttons?]",
                "1: [What color is used in the figure to indicate the predicted accessibility of each video description segment? (descriptive)]\n2: [How can authors preview the nearby narration in relation to the video description segments? (descriptive)]\n3: [What do the heights of the video description segments represent and how does that inform the authors? (descriptive)]\n4: [What options do authors have for each video description segment as shown in the figure?]\n5: [How are the video description segments and caption segments shown in relation to each other according to the figure?]\n6: [What does the alignment of video description segments with caption segments suggest about the design intent? (descriptive)]\n7: [In the context of the figure, what is the purpose of the \"save\"/\"edit\" and \"dismiss\" buttons?]",
                "No questions defined for this image."
            ],
            "questions_customized": [
                "No questions defined for this image.",
                "No questions defined for this image.",
                "No questions defined for this image.",
                "No questions defined for this image.",
                "No questions defined for this image."
            ],
            "answers": [
                "No questions defined for this image.",
                "No questions defined for this image.",
                "No questions defined for this image.",
                "No questions defined for this image.",
                "No questions defined for this image."
            ],
            "preference": [
                "No preference or request defined for this image."
            ],
            "note": [
                "No note defined for this image."
            ],
            "summary": [
                "No summary defined for this image."
            ],
            "subfigureJSON": [
                "[{\"x\":0, \"y\":0, \"width\":0, \"height\":0, \"annotation\":\"\"}]"
            ]
        },
        {
            "image": [
                "https://dl.acm.org/cms/attachment/html/10.1145/3526113.3545703/assets/html/images/uist22-91-fig3.jpg"
            ],
            "caption": [
                "Authors can hover on segments of visual/audio timelines in CrossA11y to inspect the computed correspondence between the selected segment and segments in the other modality. Here, opaque segments in the audio track are the segments predicted by the system to match the segment in the video track."
            ],
            "text": [
                "By default, the video description pane displays visual segments with estimated accessibility scores lower than 0.35 (range 0-1). Authors can use the slider (Figure\u807d2 G) to surface more visual accessibility problems when making sure they covered everything, or fewer accessibility problems when prioritizing for a time constraint."
            ],
            "questions": [
                "[questions(descriptive):1]\nWhat color is used to represent the audio track in the figure?\n\n[questions(descriptive):2]\nWhat color is used to represent the visual track in the figure?\n\n[questions(descriptive):3]\nWhat action is being indicated by the cursor in the figure?\n\n[questions:4]\nIs there a distinct pattern or markers on the audio track that differs from the visual track?\n\n[questions:5]\nAre the intensities of the audio and visual tracks represented by variations in color saturation or brightness?",
                "[questions(descriptive):1]\nWhat color is used to represent the audio track in the figure?\n\n[questions(descriptive):2]\nWhat color is used to represent the visual track in the figure?\n\n[questions(descriptive):3]\nWhat action is being indicated by the cursor in the figure?\n\n[questions:4]\nIs there a distinct pattern or markers on the audio track that differs from the visual track?\n\n[questions:5]\nAre the intensities of the audio and visual tracks represented by variations in color saturation or brightness?",
                "1: [What color is used in the figure to indicate the predicted accessibility of each video description segment? (descriptive)]\n2: [How can authors preview the nearby narration in relation to the video description segments? (descriptive)]\n3: [What do the heights of the video description segments represent and how does that inform the authors? (descriptive)]\n4: [What options do authors have for each video description segment as shown in the figure?]\n5: [How are the video description segments and caption segments shown in relation to each other according to the figure?]\n6: [What does the alignment of video description segments with caption segments suggest about the design intent? (descriptive)]\n7: [In the context of the figure, what is the purpose of the \"save\"/\"edit\" and \"dismiss\" buttons?]",
                "1: [What color is used in the figure to indicate the predicted accessibility of each video description segment? (descriptive)]\n2: [How can authors preview the nearby narration in relation to the video description segments? (descriptive)]\n3: [What do the heights of the video description segments represent and how does that inform the authors? (descriptive)]\n4: [What options do authors have for each video description segment as shown in the figure?]\n5: [How are the video description segments and caption segments shown in relation to each other according to the figure?]\n6: [What does the alignment of video description segments with caption segments suggest about the design intent? (descriptive)]\n7: [In the context of the figure, what is the purpose of the \"save\"/\"edit\" and \"dismiss\" buttons?]",
                "No questions defined for this image."
            ],
            "questions_customized": [
                "No questions defined for this image.",
                "No questions defined for this image.",
                "No questions defined for this image.",
                "No questions defined for this image.",
                "No questions defined for this image."
            ],
            "answers": [
                "No questions defined for this image.",
                "No questions defined for this image.",
                "No questions defined for this image.",
                "No questions defined for this image.",
                "No questions defined for this image."
            ],
            "preference": [
                "No preference or request defined for this image."
            ],
            "note": [
                "No note defined for this image."
            ],
            "summary": [
                "No summary defined for this image."
            ],
            "subfigureJSON": [
                "[{\"x\":0, \"y\":0, \"width\":0, \"height\":0, \"annotation\":\"\"}]"
            ]
        },
        {
            "image": [
                "https://dl.acm.org/cms/attachment/html/10.1145/3526113.3545703/assets/html/images/uist22-91-fig2.jpg"
            ],
            "caption": [
                "In CrossA11y's interface, the video pane (A) displays audio and visual timelines with accessibility visualization that allows authors to quickly identify and navigate to accessibility issues. The video description pane (F) surfaces inaccessible visual segments and lets authors to add descriptions. The captions pane (E) provides time-aligned captions and detected non-speech sound segments for authors to seek within the video and add captions."
            ],
            "text": [
                ""
            ],
            "questions": [
                "[questions(descriptive):1]\nWhat color is used to represent the audio track in the figure?\n\n[questions(descriptive):2]\nWhat color is used to represent the visual track in the figure?\n\n[questions(descriptive):3]\nWhat action is being indicated by the cursor in the figure?\n\n[questions:4]\nIs there a distinct pattern or markers on the audio track that differs from the visual track?\n\n[questions:5]\nAre the intensities of the audio and visual tracks represented by variations in color saturation or brightness?",
                "[questions(descriptive):1]\nWhat color is used to represent the audio track in the figure?\n\n[questions(descriptive):2]\nWhat color is used to represent the visual track in the figure?\n\n[questions(descriptive):3]\nWhat action is being indicated by the cursor in the figure?\n\n[questions:4]\nIs there a distinct pattern or markers on the audio track that differs from the visual track?\n\n[questions:5]\nAre the intensities of the audio and visual tracks represented by variations in color saturation or brightness?",
                "1: [What color is used in the figure to indicate the predicted accessibility of each video description segment? (descriptive)]\n2: [How can authors preview the nearby narration in relation to the video description segments? (descriptive)]\n3: [What do the heights of the video description segments represent and how does that inform the authors? (descriptive)]\n4: [What options do authors have for each video description segment as shown in the figure?]\n5: [How are the video description segments and caption segments shown in relation to each other according to the figure?]\n6: [What does the alignment of video description segments with caption segments suggest about the design intent? (descriptive)]\n7: [In the context of the figure, what is the purpose of the \"save\"/\"edit\" and \"dismiss\" buttons?]",
                "1: [What color is used in the figure to indicate the predicted accessibility of each video description segment? (descriptive)]\n2: [How can authors preview the nearby narration in relation to the video description segments? (descriptive)]\n3: [What do the heights of the video description segments represent and how does that inform the authors? (descriptive)]\n4: [What options do authors have for each video description segment as shown in the figure?]\n5: [How are the video description segments and caption segments shown in relation to each other according to the figure?]\n6: [What does the alignment of video description segments with caption segments suggest about the design intent? (descriptive)]\n7: [In the context of the figure, what is the purpose of the \"save\"/\"edit\" and \"dismiss\" buttons?]",
                "No questions defined for this image."
            ],
            "questions_customized": [
                "No questions defined for this image.",
                "No questions defined for this image.",
                "No questions defined for this image.",
                "No questions defined for this image.",
                "No questions defined for this image."
            ],
            "answers": [
                "No questions defined for this image.",
                "No questions defined for this image.",
                "No questions defined for this image.",
                "No questions defined for this image.",
                "No questions defined for this image."
            ],
            "preference": [
                "No preference or request defined for this image."
            ],
            "note": [
                "No note defined for this image."
            ],
            "summary": [
                "No summary defined for this image."
            ],
            "subfigureJSON": [
                "[{\"x\":0, \"y\":0, \"width\":0, \"height\":0, \"annotation\":\"\"}]"
            ]
        },
        {
            "image": [
                "https://dl.acm.org/cms/attachment/html/10.1145/3526113.3545703/assets/html/images/uist22-91-fig4.jpg"
            ],
            "caption": [
                "(A) After editing their description, authors can add the description to the video by clicking on \u201cSave\u201d. The vertical side bar and the horizontal bar of the corresponding segment will change to blue color, indicating that this problem has been addressed. (B) Authors can also dismiss a problem by clicking on \u201cDismiss\u201d. The bars will turn gray indicating that this problem has been dismissed."
            ],
            "text": [
                "The video description pane (Figure\u00a02 F) lets address inaccessible visual segments by writing text descriptions of the visual content. Each video description segment consists of a vertical sidebar that is colored according to its predicted accessibility, an editable text field where an author may add descriptions, and \u201csave\u201d/\u201cedit\u201d and \u201cdismiss\u201d buttons to address or ignore surfaced visual accessibility issues. Video description segments are relatively aligned with the caption segments in the captions pane such that authors can preview the nearby narration. The height of each video description segment represents its relative length such that authors can estimate the approximate length of description required."
            ],
            "questions": [
                "[questions(descriptive):1]\nWhat color is used to represent the audio track in the figure?\n\n[questions(descriptive):2]\nWhat color is used to represent the visual track in the figure?\n\n[questions(descriptive):3]\nWhat action is being indicated by the cursor in the figure?\n\n[questions:4]\nIs there a distinct pattern or markers on the audio track that differs from the visual track?\n\n[questions:5]\nAre the intensities of the audio and visual tracks represented by variations in color saturation or brightness?",
                "[questions(descriptive):1]\nWhat color is used to represent the audio track in the figure?\n\n[questions(descriptive):2]\nWhat color is used to represent the visual track in the figure?\n\n[questions(descriptive):3]\nWhat action is being indicated by the cursor in the figure?\n\n[questions:4]\nIs there a distinct pattern or markers on the audio track that differs from the visual track?\n\n[questions:5]\nAre the intensities of the audio and visual tracks represented by variations in color saturation or brightness?",
                "1: [What color is used in the figure to indicate the predicted accessibility of each video description segment? (descriptive)]\n2: [How can authors preview the nearby narration in relation to the video description segments? (descriptive)]\n3: [What do the heights of the video description segments represent and how does that inform the authors? (descriptive)]\n4: [What options do authors have for each video description segment as shown in the figure?]\n5: [How are the video description segments and caption segments shown in relation to each other according to the figure?]\n6: [What does the alignment of video description segments with caption segments suggest about the design intent? (descriptive)]\n7: [In the context of the figure, what is the purpose of the \"save\"/\"edit\" and \"dismiss\" buttons?]",
                "1: [What color is used in the figure to indicate the predicted accessibility of each video description segment? (descriptive)]\n2: [How can authors preview the nearby narration in relation to the video description segments? (descriptive)]\n3: [What do the heights of the video description segments represent and how does that inform the authors? (descriptive)]\n4: [What options do authors have for each video description segment as shown in the figure?]\n5: [How are the video description segments and caption segments shown in relation to each other according to the figure?]\n6: [What does the alignment of video description segments with caption segments suggest about the design intent? (descriptive)]\n7: [In the context of the figure, what is the purpose of the \"save\"/\"edit\" and \"dismiss\" buttons?]",
                "No questions defined for this image."
            ],
            "questions_customized": [
                "No questions defined for this image.",
                "No questions defined for this image.",
                "No questions defined for this image.",
                "No questions defined for this image.",
                "No questions defined for this image."
            ],
            "answers": [
                "No questions defined for this image.",
                "No questions defined for this image.",
                "No questions defined for this image.",
                "No questions defined for this image.",
                "No questions defined for this image."
            ],
            "preference": [
                "No preference or request defined for this image."
            ],
            "note": [
                "No note defined for this image."
            ],
            "summary": [
                "No summary defined for this image."
            ],
            "subfigureJSON": [
                "[{\"x\":0, \"y\":0, \"width\":0, \"height\":0, \"annotation\":\"\"}]"
            ]
        },
        {
            "image": [
                "https://dl.acm.org/cms/attachment/html/10.1145/3526113.3545703/assets/html/images/uist22-91-fig1.jpg"
            ],
            "caption": [
                "Our system (A) identifies accessibility issues by locating modality asymmetries (in red) between audio segments and video segments using cross-modal grounding. (B) lets authors address accessibility issues using the CrossA11y interface to write captions and video descriptions, and (C) creates a more accessible video from the authored descriptions."
            ],
            "text": [
                "We evaluated CrossA11y in a user study with 11 video authors creating captions and audio descriptions for four videos. Authors more efficiently authored audio descriptions and captions with better precision and recall in addressing accessibility issues when using CrossA11y's modality asymmetry predictions than without these predictions. We also invited two video authors who frequently posted videos on YouTube to use CrossA11y to make two of their own videos accessible, and reported that they would use CrossA11y in their workflow to produce more accessible videos."
            ],
            "questions": [
                "[questions(descriptive):1]\nWhat color is used to represent the audio track in the figure?\n\n[questions(descriptive):2]\nWhat color is used to represent the visual track in the figure?\n\n[questions(descriptive):3]\nWhat action is being indicated by the cursor in the figure?\n\n[questions:4]\nIs there a distinct pattern or markers on the audio track that differs from the visual track?\n\n[questions:5]\nAre the intensities of the audio and visual tracks represented by variations in color saturation or brightness?",
                "[questions(descriptive):1]\nWhat color is used to represent the audio track in the figure?\n\n[questions(descriptive):2]\nWhat color is used to represent the visual track in the figure?\n\n[questions(descriptive):3]\nWhat action is being indicated by the cursor in the figure?\n\n[questions:4]\nIs there a distinct pattern or markers on the audio track that differs from the visual track?\n\n[questions:5]\nAre the intensities of the audio and visual tracks represented by variations in color saturation or brightness?",
                "1: [What color is used in the figure to indicate the predicted accessibility of each video description segment? (descriptive)]\n2: [How can authors preview the nearby narration in relation to the video description segments? (descriptive)]\n3: [What do the heights of the video description segments represent and how does that inform the authors? (descriptive)]\n4: [What options do authors have for each video description segment as shown in the figure?]\n5: [How are the video description segments and caption segments shown in relation to each other according to the figure?]\n6: [What does the alignment of video description segments with caption segments suggest about the design intent? (descriptive)]\n7: [In the context of the figure, what is the purpose of the \"save\"/\"edit\" and \"dismiss\" buttons?]",
                "1: [What color is used in the figure to indicate the predicted accessibility of each video description segment? (descriptive)]\n2: [How can authors preview the nearby narration in relation to the video description segments? (descriptive)]\n3: [What do the heights of the video description segments represent and how does that inform the authors? (descriptive)]\n4: [What options do authors have for each video description segment as shown in the figure?]\n5: [How are the video description segments and caption segments shown in relation to each other according to the figure?]\n6: [What does the alignment of video description segments with caption segments suggest about the design intent? (descriptive)]\n7: [In the context of the figure, what is the purpose of the \"save\"/\"edit\" and \"dismiss\" buttons?]",
                "No questions defined for this image."
            ],
            "questions_customized": [
                "No questions defined for this image.",
                "No questions defined for this image.",
                "No questions defined for this image.",
                "No questions defined for this image.",
                "No questions defined for this image."
            ],
            "answers": [
                "No questions defined for this image.",
                "No questions defined for this image.",
                "No questions defined for this image.",
                "No questions defined for this image.",
                "No questions defined for this image."
            ],
            "preference": [
                "No preference or request defined for this image."
            ],
            "note": [
                "No note defined for this image."
            ],
            "summary": [
                "No summary defined for this image."
            ],
            "subfigureJSON": [
                "[{\"x\":0, \"y\":0, \"width\":0, \"height\":0, \"annotation\":\"\"}]"
            ]
        }
    ],
    "mainPageSpans": [
        {
            "text": "By default, the video description pane displays visual segments with estimated accessibility scores lower than 0.35 (range 0-1). Authors can use the slider (Figure\u807d2 G) to surface more visual accessibility problems when making sure they covered everything, or fewer accessibility problems when prioritizing for a time constraint.",
            "id": "highlight1",
            "imageId": "image-1711673959544"
        },
        {
            "text": "By default, the video description pane displays visual segments with estimated accessibility scores lower than 0.35 (range 0-1). Authors can use the slider (Figure\u807d2 G) to surface more visual accessibility problems when making sure they covered everything, or fewer accessibility problems when prioritizing for a time constraint.",
            "id": "highlight1",
            "imageId": "image-1711674386720"
        },
        {
            "text": "The video description pane (Figure\u00a02 F) lets address inaccessible visual segments by writing text descriptions of the visual content. Each video description segment consists of a vertical sidebar that is colored according to its predicted accessibility, an editable text field where an author may add descriptions, and \u201csave\u201d/\u201cedit\u201d and \u201cdismiss\u201d buttons to address or ignore surfaced visual accessibility issues. Video description segments are relatively aligned with the caption segments in the captions pane such that authors can preview the nearby narration. The height of each video description segment represents its relative length such that authors can estimate the approximate length of description required.",
            "id": "highlight1",
            "imageId": "image-1711674536331"
        },
        {
            "text": "Prior work aims to help people manually author audio descriptions with task-specific authoring tools\u00a0[1, 6, 16], feedback on the content at production-time\u00a0[33], with feedback on audio descriptions\u00a0[27, 39], and with hosted descriptions\u00a0[16]. Since authoring descriptions is a time-consuming process, other prior work seeks to provide computational support for this task including using: computer vision to detect visual content\u00a0[8, 12, 13], using deep learning to provide a computer-drafted description\u00a0[13, 46, 48], synthesized voice to convert text to speech\u00a0[12, 17, 18, 41], and automatic editing to fit human-authored descriptions into the space provided\u00a0[31]. While focusing on methods to help people write better descriptions, such tools only find inaccessible moments for description by surfacing silent portions of the video\u00a0[6, 12, 31, 48], or by helping people find film-specific visual content that may need descriptions (e.g., scene changes, characters\u00a0[12]). Rather than assessing video in a single modality, we explore finding accessibility problems by assessing asymmetries between the auditory and visual content.",
            "id": "highlight1",
            "imageId": "image-1711694953113"
        },
        {
            "text": "Assessing accessibility of visual content is challenging for authors who do not share accessibility needs with their audience members. As a result, accessibility research includes a long history of prior work aimed to help people assess and correct accessibility problems in their designs including tools aimed at simulating accessibility issues\u00a0[4, 5] and evaluating accessibility with respect to metrics\u00a0[24, 43]. Simulation-style tools to support sighted designers trying to achieve visually accessible designs; for example, Chrome Dev Tool's colorblindness and blurriness emulators to help designers assess legibility\u00a0[5]. Using such simulations as a replacement for involvement with people with disabilities has several issues, as they are unable to capture the full experience of disability and give designers false conceptions\u00a0[42]. Given that people with disabilities, in partnership with organizations (e.g., W3C\u00a0[7]), have authored guidelines and best practices to make design accessible, other prior work alerts authors to violations of these guidelines in authoring tools. For example, accessibility checkers in PowerPoint\u00a0[34] and Adobe Acrobat\u00a0[35] alert authors to potential accessibility issues in their designs (e.g., missing alt text, document read order). Furthermore, web accessibility checkers provide a report card on similar types of issues to fix\u00a0[24, 43]. We extend prior work by assessing the accessibility issues and surfacing accessibility issues based on existing guidelines about video accessibility.",
            "id": "highlight2",
            "imageId": "image-1711694953113"
        },
        {
            "text": "We evaluated CrossA11y in a user study with 11 video authors creating captions and audio descriptions for four videos. Authors more efficiently authored audio descriptions and captions with better precision and recall in addressing accessibility issues when using CrossA11y's modality asymmetry predictions than without these predictions. We also invited two video authors who frequently posted videos on YouTube to use CrossA11y to make two of their own videos accessible, and reported that they would use CrossA11y in their workflow to produce more accessible videos.",
            "id": "highlight1",
            "imageId": "image-1711697224305"
        },
        {
            "text": "The video pane (Figure\u00a02 A) displays the video and lets authors play/pause the video and seek within the video using two timelines: (1) the audio timeline that lets authors navigate to auditorily inaccessible segments, and (2) the visual timeline that lets authors navigate to visually inaccessible segments. The audio timeline displays audio segments that each represent a segment with continuous speech, or non-speech sound. The visual timeline displays visual segments that each represent a segment of continuous footage (i.e., a shot). Each segment is colored with its estimated accessibility1 from gray (accessible) to red (inaccessible) using sRGB inverse gamma mixing. The darkness of the red represents the weighted sum of the similarity score of a segment to segments in the other modality. Using either timeline, authors can gain an overview of accessibility issues, or quickly navigate to an inaccessible segment by clicking on the segment to play the corresponding point in the video. For example, by clicking on the first red segment in the audio track (Figure\u00a02 C) an author will hear an inaccessible audio segment \u2014 music plays that is not available in the captions preview (Figure\u00a02 D). Authors may inspect inaccessibility prediction results displayed in the timeline by hovering over an audio or visual segment (Figure\u00a03) to see the audio segments that are predicted to match that segment (displayed with a higher opacity). As the author navigates and plays the video with the video pane, the corresponding segments are highlighted in the linked video description pane and the caption pane.",
            "id": "highlight1",
            "imageId": "image-1712511961890"
        },
        {
            "text": "The video description pane (Figure\u00a02 F) lets address inaccessible visual segments by writing text descriptions of the visual content. Each video description segment consists of a vertical sidebar that is colored according to its predicted accessibility, an editable text field where an author may add descriptions, and \u201csave\u201d/\u201cedit\u201d and \u201cdismiss\u201d buttons to address or ignore surfaced visual accessibility issues. Video description segments are relatively aligned with the caption segments in the captions pane such that authors can preview the nearby narration. The height of each video description segment represents its relative length such that authors can estimate the approximate length of description required.",
            "id": "highlight2",
            "imageId": "image-1712511988762"
        }
    ],
    "sidePanelSpans": [
        {
            "text": "By default, the video description pane displays visual segments with estimated accessibility scores lower than 0.35 (range 0-1). Authors can use the slider (Figure\u807d2 G) to surface more visual accessibility problems when making sure they covered everything, or fewer accessibility problems when prioritizing for a time constraint.",
            "id": "highlight1"
        },
        {
            "text": "By default, the video description pane displays visual segments with estimated accessibility scores lower than 0.35 (range 0-1). Authors can use the slider (Figure\u807d2 G) to surface more visual accessibility problems when making sure they covered everything, or fewer accessibility problems when prioritizing for a time constraint.",
            "id": "highlight1"
        },
        {
            "text": "The video description pane (Figure\u00a02 F) lets address inaccessible visual segments by writing text descriptions of the visual content. Each video description segment consists of a vertical sidebar that is colored according to its predicted accessibility, an editable text field where an author may add descriptions, and \u201csave\u201d/\u201cedit\u201d and \u201cdismiss\u201d buttons to address or ignore surfaced visual accessibility issues. Video description segments are relatively aligned with the caption segments in the captions pane such that authors can preview the nearby narration. The height of each video description segment represents its relative length such that authors can estimate the approximate length of description required.",
            "id": "highlight1"
        },
        {
            "text": "Prior work aims to help people manually author audio descriptions with task-specific authoring tools\u00a0[1, 6, 16], feedback on the content at production-time\u00a0[33], with feedback on audio descriptions\u00a0[27, 39], and with hosted descriptions\u00a0[16]. Since authoring descriptions is a time-consuming process, other prior work seeks to provide computational support for this task including using: computer vision to detect visual content\u00a0[8, 12, 13], using deep learning to provide a computer-drafted description\u00a0[13, 46, 48], synthesized voice to convert text to speech\u00a0[12, 17, 18, 41], and automatic editing to fit human-authored descriptions into the space provided\u00a0[31]. While focusing on methods to help people write better descriptions, such tools only find inaccessible moments for description by surfacing silent portions of the video\u00a0[6, 12, 31, 48], or by helping people find film-specific visual content that may need descriptions (e.g., scene changes, characters\u00a0[12]). Rather than assessing video in a single modality, we explore finding accessibility problems by assessing asymmetries between the auditory and visual content.",
            "id": "highlight1"
        },
        {
            "text": "Assessing accessibility of visual content is challenging for authors who do not share accessibility needs with their audience members. As a result, accessibility research includes a long history of prior work aimed to help people assess and correct accessibility problems in their designs including tools aimed at simulating accessibility issues\u00a0[4, 5] and evaluating accessibility with respect to metrics\u00a0[24, 43]. Simulation-style tools to support sighted designers trying to achieve visually accessible designs; for example, Chrome Dev Tool's colorblindness and blurriness emulators to help designers assess legibility\u00a0[5]. Using such simulations as a replacement for involvement with people with disabilities has several issues, as they are unable to capture the full experience of disability and give designers false conceptions\u00a0[42]. Given that people with disabilities, in partnership with organizations (e.g., W3C\u00a0[7]), have authored guidelines and best practices to make design accessible, other prior work alerts authors to violations of these guidelines in authoring tools. For example, accessibility checkers in PowerPoint\u00a0[34] and Adobe Acrobat\u00a0[35] alert authors to potential accessibility issues in their designs (e.g., missing alt text, document read order). Furthermore, web accessibility checkers provide a report card on similar types of issues to fix\u00a0[24, 43]. We extend prior work by assessing the accessibility issues and surfacing accessibility issues based on existing guidelines about video accessibility.",
            "id": "highlight2"
        },
        {
            "text": "We evaluated CrossA11y in a user study with 11 video authors creating captions and audio descriptions for four videos. Authors more efficiently authored audio descriptions and captions with better precision and recall in addressing accessibility issues when using CrossA11y's modality asymmetry predictions than without these predictions. We also invited two video authors who frequently posted videos on YouTube to use CrossA11y to make two of their own videos accessible, and reported that they would use CrossA11y in their workflow to produce more accessible videos.",
            "id": "highlight1"
        },
        {
            "text": "The video pane (Figure\u00a02 A) displays the video and lets authors play/pause the video and seek within the video using two timelines: (1) the audio timeline that lets authors navigate to auditorily inaccessible segments, and (2) the visual timeline that lets authors navigate to visually inaccessible segments. The audio timeline displays audio segments that each represent a segment with continuous speech, or non-speech sound. The visual timeline displays visual segments that each represent a segment of continuous footage (i.e., a shot). Each segment is colored with its estimated accessibility1 from gray (accessible) to red (inaccessible) using sRGB inverse gamma mixing. The darkness of the red represents the weighted sum of the similarity score of a segment to segments in the other modality. Using either timeline, authors can gain an overview of accessibility issues, or quickly navigate to an inaccessible segment by clicking on the segment to play the corresponding point in the video. For example, by clicking on the first red segment in the audio track (Figure\u00a02 C) an author will hear an inaccessible audio segment \u2014 music plays that is not available in the captions preview (Figure\u00a02 D). Authors may inspect inaccessibility prediction results displayed in the timeline by hovering over an audio or visual segment (Figure\u00a03) to see the audio segments that are predicted to match that segment (displayed with a higher opacity). As the author navigates and plays the video with the video pane, the corresponding segments are highlighted in the linked video description pane and the caption pane.",
            "id": "highlight1"
        },
        {
            "text": "The video description pane (Figure\u00a02 F) lets address inaccessible visual segments by writing text descriptions of the visual content. Each video description segment consists of a vertical sidebar that is colored according to its predicted accessibility, an editable text field where an author may add descriptions, and \u201csave\u201d/\u201cedit\u201d and \u201cdismiss\u201d buttons to address or ignore surfaced visual accessibility issues. Video description segments are relatively aligned with the caption segments in the captions pane such that authors can preview the nearby narration. The height of each video description segment represents its relative length such that authors can estimate the approximate length of description required.",
            "id": "highlight2"
        }
    ]
}