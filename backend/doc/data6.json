{
  "dataset": [
    {
      "image": [
        "https://dl.acm.org/cms/attachment/html/10.1145/3526113.3545703/assets/html/images/uist22-91-fig1.jpg"
      ],
      
      "caption": [
        "Our system (A) identifies accessibility issues by locating modality asymmetries (in red) between audio segments and video segments using cross-modal grounding. (B) lets authors address accessibility issues using the CrossA11y interface to write captions and video descriptions, and (C) creates a more accessible video from the authored descriptions."
      ],
      "text": [
        "We evaluated CrossA11y in a user study with 11 video authors creating captions and audio descriptions for four videos. Authors more efficiently authored audio descriptions and captions with better precision and recall in addressing accessibility issues when using CrossA11y's modality asymmetry predictions than without these predictions. We also invited two video authors who frequently posted videos on YouTube to use CrossA11y to make two of their own videos accessible, and reported that they would use CrossA11y in their workflow to produce more accessible videos."
      ],
      "questions": [
        "No questions defined for this image."
      ],
      "questions_customized": [
        "No questions from the user defined for this image."
      ],
      "answers": [
        "No answers to generated questions defined for this image."
      ],
      "preference": [
        "No preference or request defined for this image."
      ],
      "note": [
        "No note defined for this image."
      ],
      "summary": [
        "No summary defined for this image."
      ],
      "subfigureJSON": [
        "[{\"x\":0, \"y\":0, \"width\":0, \"height\":0, \"annotation\":\"\"}]"
      ]
    }
  ],
  "mainPageSpans": [
    {
      "text": "We evaluated CrossA11y in a user study with 11 video authors creating captions and audio descriptions for four videos. Authors more efficiently authored audio descriptions and captions with better precision and recall in addressing accessibility issues when using CrossA11y's modality asymmetry predictions than without these predictions. We also invited two video authors who frequently posted videos on YouTube to use CrossA11y to make two of their own videos accessible, and reported that they would use CrossA11y in their workflow to produce more accessible videos.",
      "id": "highlight1",
      "imageId": "image-1711697224305"
    }
  ],
  "sidePanelSpans": [
    {
      "text": "We evaluated CrossA11y in a user study with 11 video authors creating captions and audio descriptions for four videos. Authors more efficiently authored audio descriptions and captions with better precision and recall in addressing accessibility issues when using CrossA11y's modality asymmetry predictions than without these predictions. We also invited two video authors who frequently posted videos on YouTube to use CrossA11y to make two of their own videos accessible, and reported that they would use CrossA11y in their workflow to produce more accessible videos.",
      "id": "highlight1"
    }
  ]
}